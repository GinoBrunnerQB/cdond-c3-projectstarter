version: 2.1

commands:
  destroy-environment:
    description: Delete files from s3 bucket and destroy back-end and front-end cloudformation stacks
    steps:
      - run:
          name: Delete files from s3 bucket
          when: on_fail
          command: |
            aws s3 rm "s3://udapeople-${CIRCLE_WORKFLOW_ID:0:7}" --recursive
      - run:
          name: Destroy stacks
          when: on_fail
          command: |
            aws cloudformation delete-stack --stack-name "udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}"
            aws cloudformation delete-stack --stack-name "udapeople-frontend-${CIRCLE_WORKFLOW_ID:0:7}"


  revert-migrations:
    description: Revert the last migration if successfully run in the current workflow.
    #parameters:
      # Add parameter here     
    steps:
      - run:
          name: Revert migrations
          when: on_fail
          command: |
            # Curl command here to see if there was a successful migration associated with the workflow id, store result in SUCCESS variable
            SUCCESS=$(curl -k https://kvdb.io/AgDSCaSFfxFuHcGeNmMMTx/migration_${CIRCLE_WORKFLOW_ID:0:7})
            if(( $SUCCESS==1 )); 
            then
              cd ~/project/backend
              npm install
              npm run migrations:revert
            fi
            
jobs:
  build-frontend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-build]
      - run:
          name: Build front-end
          command: cd frontend && npm i && npm run build
      - save_cache:
          paths: [frontend/node_modules]
          key: frontend-build

  build-backend:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [backend-build]
      - run:
          name: Back-end build
          command: cd backend && npm i && npm run build
      - save_cache:
          paths: [backend/node_modules]
          key: backend-build

  test-backend:
    docker:
       - image: circleci/node:13.8.0
    steps:
       - checkout
       - restore_cache:
           keys: [backend-build]
       - run:
           name: Install npm dependencies
           command: cd backend && npm install
       - run:
           name: Run back-end tests
           command: cd backend && npm run test
       - save_cache:
           paths: [ backend/node_modules ]
           key: backend-build
                
  test-frontend:
    docker:
       - image: circleci/node:13.8.0
    steps:
       - checkout
       - restore_cache:
           keys: [frontend-build]
       - run:
           name: Install npm dependencies
           command: cd frontend && npm install
       - run:
           name: Run front-end tests
           command: cd frontend && npm run test
       - save_cache:
           paths: [ frontend/node_modules ]
           key: frontend-build
            
  scan-frontend:
     docker:
       - image: circleci/node:13.8.0
     steps:
       - checkout
       - restore_cache:
           keys: [frontend-build]
       - run:
           name: Install npm dependencies
           command: cd frontend && npm install
       - run:
           name: Run vulnerability check
           command: cd frontend && npm audit fix --audit-level=critical -- force

  scan-backend:
     docker:
       - image: circleci/node:13.8.0
     steps:
       - checkout
       - restore_cache:
           keys: [backend-build]
       - run:
           name: Install npm dependencies
           command: cd backend && npm install
       - run:
           name: Run vulnerability check
           command: cd backend && npm audit fix --audit-level=critical -- force

  deploy-infrastructure:
     docker:
       - image: amazon/aws-cli
     steps:
       - checkout
       - run:
           name: Install tar utility
           command: |
               yum install -y tar gzip
       - run:
           name: Ensure back-end infrastructure exists
           command: |
               aws cloudformation deploy \
               --template-file .circleci/files/backend.yml \
               --tags project=backend \
               --stack-name "udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}" \
               --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"
       - run:
           name: Ensure front-end infrastructure exist
           command: |
               aws cloudformation deploy \
               --template-file .circleci/files/frontend.yml \
               --tags project=frontend \
               --stack-name "udapeople-frontend-${CIRCLE_WORKFLOW_ID:0:7}" \
               --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"
#       - run:
#           name: Deploy cloudfront distribution
#           command: |
#             aws cloudformation deploy \
#              --template-file .circleci/files/cloudfront.yml \
#              --stack-name "udapeople-cloudfront-${CIRCLE_WORKFLOW_ID:0:7}" \
#              --parameter-overrides WorkflowID="${CIRCLE_WORKFLOW_ID:0:7}" \
#              --tags project=udapeople
       - run:
           name: Add back-end ip to ansible inventory
           command: |
             aws ec2 describe-instances \
                 --filters Name=tag:Name,Values="backend-${CIRCLE_WORKFLOW_ID:0:7}" \
                 --query 'Reservations[*].Instances[*].{url:PublicIpAddress}' \
                 --output text >> .circleci/ansible/inventory.txt
       - persist_to_workspace:
           root: .
           paths:
             - .circleci/ansible/inventory.txt
       - destroy-environment

  configure-infrastructure:
     docker:
       - image: python:3.7-alpine3.11
     steps:
       - checkout
       - add_ssh_keys:
           fingerprints:
             - "9d:40:b8:cc:27:80:fa:24:1f:ae:e6:e1:f6:59:8d:3c"
       - attach_workspace:
          at: .
       - run:
           name: Install dependencies
           command: |
             apk add --update ansible && apk add --update openssh
       - run:
           name: Update known hosts file
           command: |
              cat .circleci/ansible/inventory.txt | egrep "^[0-9]" > filtered.txt &&
              ssh-keyscan -T 10 -H -f filtered.txt >> ~/.ssh/known_hosts
       - run:
           name: Configure server
           command: |
             echo ENVIROMENT=production > "backend/.env"
             echo TYPEORM_CONNECTION=postgres >> "backend/.env"
             echo TYPEORM_ENTITIES=./src/modules/domain/*/*/*.entity.ts >> "backend/.env"
             echo TYPEORM_HOST=$TYPEORM_HOST >> "backend/.env"
             echo TYPEORM_PORT=$TYPEORM_PORT >> "backend/.env"
             echo TYPEORM_USERNAME=$TYPEORM_USERNAME >> "backend/.env"
             echo TYPEORM_MIGRATIONS=$TYPEORM_MIGRATIONS >> "backend/.env"
             echo TYPEORM_MIGRATIONS_DIR=$TYPEORM_MIGRATIONS_DIR >> "backend/.env"
             echo TYPEORM_PASSWORD=$TYPEORM_PASSWORD >> "backend/.env"
             echo TYPEORM_DATABASE=$TYPEORM_DATABASE >> "backend/.env"
             cat backend/.env
             ansible-playbook .circleci/ansible/configure-server.yml -i .circleci/ansible/inventory.txt
       - destroy-environment

  run-migrations:
     docker:
       - image: circleci/node:13.8.0
     steps:
       - checkout
       - restore_cache:
           keys: [backend-build]
       - run:
           name: Install npm dependencies
           command: |
              cd backend
              npm install
       - run:
           name: Run migrations
           no_output_timeout: 10m
           command: |
             cd backend
             npm run migrations | tee ~/project/backend/migration_log.txt
       - run:
           name: Send migration results to kvdb
           command: |
             if grep -q "No migrations are pending"  ~/project/backend/migration_log.txt
             then
                curl -k https://kvdb.io/AgDSCaSFfxFuHcGeNmMMTx/migration_${CIRCLE_WORKFLOW_ID:0:7}  -d 1
             else
                curl -k https://kvdb.io/AgDSCaSFfxFuHcGeNmMMTx/migration_${CIRCLE_WORKFLOW_ID:0:7}  -d 0
             fi
       - destroy-environment
       - revert-migrations



  deploy-frontend:
     docker:
       - image: amazon/aws-cli
     steps:
       - checkout
       - restore_cache:
           keys: [frontend-build]
       - run:
           name: Install dependencies
           command: |
               curl --silent --location https://rpm.nodesource.com/setup_14.x | bash -
               yum install -y tar gzip nodejs
       - attach_workspace:
           at: .
       - run:
           name: Get backend url
           command: |
             export BACKEND_IP=$(aws ec2 describe-instances \
                              --filters Name=tag:Name,Values="backend-${CIRCLE_WORKFLOW_ID:0:7}" \
                              --query 'Reservations[*].Instances[*].{url:PublicIpAddress}' \
                              --output text) \
             export API_URL="https://${BACKEND_IP}:3030"
             echo "${API_URL}"
       - run:
           name: Deploy frontend objects
           command: |
             cd frontend
             npm i
             npm run build
             tar -czvf artifact-"${CIRCLE_WORKFLOW_ID:0:7}".tar.gz dist
             aws s3 sync dist s3://udapeople-${CIRCLE_WORKFLOW_ID:0:7}
       - destroy-environment
       - revert-migrations
                    
  deploy-backend:
     docker:
       - image: python:3.7-alpine3.11
     steps:
       - checkout
       - restore_cache:
           keys: [backend-build]
       - add_ssh_keys:
           fingerprints:
             - "9d:40:b8:cc:27:80:fa:24:1f:ae:e6:e1:f6:59:8d:3c"
       - attach_workspace:
          at: .
       - run:
           name: Install dependencies
           command: |
             apk add --update ansible openssh npm tar gzip nodejs curl
             pip install awscli
       - run:
           name: Update known hosts file
           command: |
              cat .circleci/ansible/inventory.txt | egrep "^[0-9]" > filtered.txt &&
              ssh-keyscan -T 10 -H -f filtered.txt >> ~/.ssh/known_hosts
       - run:
           name: Deploy backend
           command: |
             echo ENVIROMENT=production > "backend/.env"
             echo TYPEORM_CONNECTION=postgres >> "backend/.env"
             echo TYPEORM_ENTITIES=./src/modules/domain/*/*/*.entity.ts >> "backend/.env"
             echo TYPEORM_HOST=$TYPEORM_HOST >> "backend/.env"
             echo TYPEORM_PORT=$TYPEORM_PORT >> "backend/.env"
             echo TYPEORM_USERNAME=$TYPEORM_USERNAME >> "backend/.env"
             echo TYPEORM_MIGRATIONS=$TYPEORM_MIGRATIONS >> "backend/.env"
             echo TYPEORM_MIGRATIONS_DIR=$TYPEORM_MIGRATIONS_DIR >> "backend/.env"
             echo TYPEORM_PASSWORD=$TYPEORM_PASSWORD >> "backend/.env"
             echo TYPEORM_DATABASE=$TYPEORM_DATABASE >> "backend/.env"
             cat backend/.env
             tar -C backend -czvf artifact.tar.gz .
             ansible-playbook -vvvv .circleci/ansible/deploy-backend.yml -i .circleci/ansible/inventory.txt
       - destroy-environment
       - revert-migrations

  smoke-test:
     docker:
       - image: python:3.7-alpine3.11
     steps:
       # Checkout code from git
       - checkout
       - run:
           name: Install dependencies
           command: |
             apk add --update curl nodejs npm
             pip install awscli
       - run:
           name: Get backend IP and run smoke test
           command: |
             export BACKEND_IP=$(aws ec2 describe-instances \
                              --filters Name=tag:Name,Values="backend-${CIRCLE_WORKFLOW_ID:0:7}" \
                              --query 'Reservations[*].Instances[*].PublicIpAddress' \
                              --output text)
             echo "${BACKEND_IP}"
             if curl "http://${BACKEND_IP}:3030/api/status" | grep "ok"
             then
               return 0
             else
               return 1
             fi
       - run:
           name: Frontend smoke test.
           command: |
             URL="http://udapeople-${CIRCLE_WORKFLOW_ID:0:7}.s3-website.eu-central-1.amazonaws.com"
             if curl ${URL} | grep "Welcome"
             then
                return 0
             else
                return 1
             fi
#       - destroy-environment
#       - revert-migrations


  cloudfront-update:
     docker:
       - image: amazon/aws-cli
     steps:
       # Checkout code from git
       - checkout
       - run:
           name: Install tar utility
           command: |
               yum install -y tar gzip
       - run:
           name: Get old workflow ID
           command: |
             aws cloudformation \
             list-exports --query "Exports[?Name==\`WorkflowID\`].Value" \
             --no-paginate --output text > oldWorkflowId.txt
             cat oldWorkflowId.txt
       - run:
          name: Update cloudfront distribution
          command: |
            aws cloudformation deploy \
            --template-file ./files/cloudfront.yml \
            --stack-name production-udapeople-frontend \
            --parameter-overrides WorkflowID="${CIRCLE_WORKFLOW_ID:0:7}"
#       - run:
#           name: Update cloudfront distribution
#           command: |
#              aws cloudformation update-stack \
#              --use-previous-template \
#              --stack-name udapeople-cloudfront\
#              --parameters ParameterKey=WorkflowID,ParameterValue=${CIRCLE_WORKFLOW_ID:0:7}
       - persist_to_workspace:
           root: .
           paths:
             - oldWorkflowId.txt
       # Here's where you will add some code to rollback on failure

  cleanup:
      docker:
        - image: amazon/aws-cli
      steps:
        - checkout
        - run:
           name: Install tar utility
           command: |
               yum install -y tar gzip
        - attach_workspace:
            at: .
        - run:
            name: Get old stack workflow id
            command: |
              export OldWorkflowID=$(cat oldWorkflowId.txt)
              export STACKS=($(aws cloudformation list-stacks --query "StackSummaries[*].StackName" \
                --stack-status-filter CREATE_COMPLETE --no-paginate --output text))
        - run:
            name: Remove old stacks and files
            command: |
              if [[ "${STACKS[@]}" =~ "${OldWorkflowID}" ]]
              then
                aws s3 rm "s3://udapeople-${OldWorkflowID}" --recursive
                aws cloudformation delete-stack --stack-name "udapeople-backend-${OldWorkflowID}"
                aws cloudformation delete-stack --stack-name "udapeople-frontend-${OldWorkflowID}"
              fi
            

workflows:
  default:
    jobs:
      - build-frontend
      - build-backend
#      - test-frontend:
#           requires: [build-frontend]
#      - test-backend:
#           requires: [build-backend]
#      - scan-backend:
#           requires: [build-backend]
#      - scan-frontend:
#           requires: [build-frontend]
      - deploy-infrastructure:
            requires: [build-frontend, build-backend]
#           requires: [test-frontend, test-backend, scan-frontend, scan-backend]
#           filters:
#             branches:
#               only: [test-feature-branch]
      - configure-infrastructure:
           requires: [deploy-infrastructure]
      - run-migrations:
           requires: [configure-infrastructure]
      - deploy-frontend:
           requires: [run-migrations]
      - deploy-backend:
           requires: [run-migrations]
      - smoke-test:
           requires: [deploy-backend, deploy-frontend]
      - cloudfront-update:
           requires: [smoke-test]
      - cleanup:
           requires: [cloudfront-update]